# ğŸ§  SLM Assistant (Small Language Model Assistant)

This project implements a **Teacher-Student Knowledge Distillation pipeline** for training a small language model (SLM) using instruction-following data. The teacher model is a large LLaMA-based model from Hugging Face, and the student is trained on a distilled dataset to make it lightweight and suitable for **edge devices**.

---

## ğŸ“‚ Project Structure

```
SLM_ASSISTANT/
â”‚â”€â”€ teacher_response/         # Teacher model inference and dataset generation
â”‚   â”‚â”€â”€ teacher_model.py      # Loads teacher model with quantization
â”‚   â”‚â”€â”€ get_response.py       # Functions to query teacher responses
â”‚   â”‚â”€â”€ teacher_response.py   # Main script to generate distillation datasets
â”‚   â”‚â”€â”€ __init__.py
â”‚
â”‚â”€â”€ utils/                    # Utility functions
â”‚   â”‚â”€â”€ logger_config.py      # Logger setup for the whole project
â”‚   â”‚â”€â”€ __init__.py
â”‚
â”‚â”€â”€ student/                  # (Planned) Student training and evaluation pipeline
â”‚
â”‚â”€â”€ teacher_datasets/         # Saved knowledge distillation datasets (.jsonl)
â”‚
â”‚â”€â”€ README.md                 # Project documentation
```

---

## ğŸš€ Features

- âœ… Loads **Meta LLaMA-3.1 8B Instruct** as the teacher model  
- âœ… Generates **teacher-student distillation datasets** from Alpaca  
- âœ… Saves datasets in **JSONL format** (knowledge distillation + mixed dataset)  
- âœ… Supports **quantization with 4-bit (bitsandbytes)** for efficient inference  
- âœ… Logging system for clean, trackable pipeline execution  

---

## âš™ï¸ Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/YOUR_GITHUB/SLM_ASSISTANT.git
   cd SLM_ASSISTANT
   ```

2. Create and activate a virtual environment:
   ```bash
   python -m venv venv
   source venv/bin/activate   # On Linux/Mac
   venv\Scripts\activate    # On Windows
   ```

3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

4. Set your **Hugging Face Token** in `.env`:
   ```bash
   HF_TOKEN=your_huggingface_token_here
   ```

---

## ğŸ“Š Usage

Run the dataset generation script from the project root:
```bash
python -m teacher_main.py
```

This will:
- Load the teacher model (`meta-llama/Llama-3.1-8B-Instruct`)
- Query Alpaca dataset samples
- Save teacher responses into `teacher_datasets/`

Example dataset entry (`alpaca_mskd_chunk0.jsonl`):
```json
{
  "instruction": "Give three tips for staying healthy.",
  "input": "",
  "ground_truth": "1. Eat a balanced diet ...",
  "teacher_response": "1. Drink plenty of water..."
}
```

---

## ğŸ”® Next Steps

- Train the **student model** on the distilled dataset  
- Evaluate student vs teacher responses  
- Optimize for **edge deployment**  

---

## ğŸ“œ License

Developed by **Yogesh Murala**
